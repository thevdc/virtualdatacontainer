---
layout: default
---

<body>
  
  <h1> Welcome to Virtual Data Container's Official Page </h1>
  Here you will have all the access to the VDC's code, the Blueprint-Artifact Editor's code, as well as their latest releases.
  
  <img src="VDC-logo.png" alt="hi" class="inline"/>
  
  <h2> What is a VDC? </h2>
  The Virtual Data Container (shortly refered to as "VDC") represents a new way of presenting the data. As the modern world constantly evolves, data are becoming a really valuable asset. During the past decade, the total amount of data generated by the world has skyrocketed. It seems only reasonable to move on to new ways of handling data, but also presenting / selling them as products. What if we were able to choose among various services, applications that suit our needs, simply presented and easily organised inside a  virtual container? What if, choosing the best apps and services was just as simple as choosing a product from the Super Market? The Virtual Data Container aims to become exactly that. A new way of presenting data, services and applications, as mentioned earlier.
  <br> <br>
  In a few words, a Virtual Data Container could be a set of data, a web service, an operating system, an application etc., organised in a virtual container image and presented to the potential buyers / users. This is the future, transforming existing operations and datasets to services and making them easily accesible to the end users. A VDC is versatile, easily upgradable, and can be tailored to a specific customers' needs. However, in case a VDC is structured as a concrete and preset / prefixed service, potential users can find out if a its good for them or not.
  <br> <br>
  As data can be distributed among resources both on the Cloud and the Edge, Virtual Data Containers (the VDCs) are proposed as a mean for timely and securely offering data also transparently with respect to their location and format.
  In more detail, a VDC:
  <ul>
  <li>Provides uniform access to data sources regardless of where they run, i.e., on the edge or on the cloud.</li>
  <li>Embeds a set of data processing techniques able to transform data (e.g., encryption, compression).</li>
  <li>Allows composing these processing techniques in pipelines (inspired by the node-RED programming model) and executing the resulting application.</li>
  <li>Can be easily deployed on resources which can live either on the edge or in the cloud.</li>
  </ul>
  
  <h2> Design & Development </h2>
  
  <img src="VDC-simple-architecture.png" alt="hi" class="inline"/>
  
  The first step of an application life-cycle concerns the work performed by a data administrator (a.k.a. data provider) who, based on the managed data sources, 
creates a VDC JSON-Schema (“Artifact” or “Blueprint”) which specifies the characteristics of a VDC in terms of following:
  <ul>
  <li>The exposed data sources.</li>
  <li>The exposed APIs.</li>
  <li>How the data from the data sources needs to be processed in order to make them available through the API.</li>
  <li>The non-functional properties defining the quality of data and service.</li>
  <li>The components cookbook: a script defining the modules composing the container as well as their deployment.</li>
</ul> 
  In this GitHub repository, we provide a VDC Schema creator, for the data providers to be able to easily introduce themselves to the VDC “network”. That we will analyse later on.
<br><br>
Following the Service-Oriented Computing principles, the visibility principle requires to publish a description of a service to make it visible to all the
potential users. As a consequence, the data administrator publishes the VDC Artifact. Once published, the developers come into play. As the
information included in a VDC Blueprint-Artifact also concerns functional and non-functional aspects, a developer relies on this information to select the most
suitable VDC according to its purposes. It is worth noticing that, based on the nature of his/her needs, the developer could select different VDCs referring to different
purposes. Finally, the developer designs and develops the software and deploys it on the available resources which can be located on the edge or in the cloud. The initial
deployment is the key element in the approach; as in this phase, it is required to know which are all the possible resources on which the VDC can be executed. A standard Fog environment implies that DaaS can be provided using resources belonging to both the provider and the consumer. Without loss of generality, we can assume that the provider resources are always in the cloud, while the consumer resources are always on the edge. In this way, a VDC living in the cloud has more capacity and it probably lives close to the data source to which it is connected. Conversely, a VDC living on the edge has the advantage of living closer to the user, thus reducing latency when providing the requested
data. Deciding where to deploy the VDC depends on the resources required by the VDC (e.g., it might happen that the amount of resources to process the data before
making them available to the user cannot be provided at the edge), the network characteristics (e.g., the connection at the consumer side can support a high-rate
transmission), and security (e.g., not all the data can be moved to the consumer side, thus even the processing cannot be placed at the edge).
  
</body>


